{
  "org_hint": "Search relevance team",
  "role_target": "Senior Applied Scientist",
  "role_requirements": {
    "critical_skills": [
      "deep learning for ranking",
      "large-scale feature engineering",
      "search relevance metrics & experimentation",
      "semantic understanding of queries"
    ]
  },
  "profiles": [
    {
      "name": "Alice",
      "skills": [
        {
          "skill": "search relevance metrics & experimentation",
          "confidence": 0.95,
          "relevance_to_role": 1.0,
          "evidence": [
            {
              "source": "github_commit",
              "id": "commit-2",
              "snippet": "Add AB test framework for ranking experiments",
              "timestamp": "2024-10-10"
            }
          ]
        },
        {
          "skill": "search relevance metrics & experimentation",
          "confidence": 0.9,
          "relevance_to_role": 1.0,
          "evidence": [
            {
              "source": "rfc",
              "id": "rfc-1",
              "snippet": "Defines offline metrics and evaluation harness for relevance experiments",
              "timestamp": "2024-09-15"
            }
          ]
        },
        {
          "skill": "large-scale feature engineering",
          "confidence": 0.7,
          "relevance_to_role": 0.8,
          "evidence": [
            {
              "source": "github_commit",
              "id": "commit-1",
              "snippet": "Refactor BM25 scorer and improve ranking evaluation",
              "timestamp": "2024-10-02"
            }
          ]
        }
      ],
      "growth_plan": [
        "Lead an end-to-end project that upgrades the existing data-pipeline to generate large-scale, query-level ranking features (n-grams, embeddings, CTR stats) for a deep-learning ranking model; document the pipeline design, performance benchmarks, and lessons learned in an internal tech blog to showcase large-scale feature engineering expertise.",
        "Partner with the search relevance team to design and run an A/B test that compares a Transformer-based semantic re-ranker against the current BM25 baseline; own the experimentation protocol (hypothesis, power analysis, metric selection using NDCG@10, MRR, and online business KPIs) and present the statistically significant results at the next company-wide ML guild meeting.",
        "Enroll in the ‘Advanced Deep Learning for Search & Recommendations’ nano-degree and apply each module by iteratively fine-tuning a BERT cross-encoder on proprietary query-product pairs; publish the converged model, code, and semantic-query-understanding ablation studies to the internal model zoo, then host a lunch-and-learn to mentor peers and cement Senior Applied Scientist visibility."
      ]
    },
    {
      "name": "Bob",
      "skills": [
        {
          "skill": "search relevance metrics & experimentation",
          "confidence": 0.9,
          "relevance_to_role": 0.95,
          "evidence": [
            {
              "source": "jira_ticket",
              "id": "ticket-1",
              "snippet": "Implement NDCG and offline metrics checks for ranking models",
              "timestamp": "2024-09-10"
            }
          ]
        },
        {
          "skill": "large-scale feature engineering",
          "confidence": 0.8,
          "relevance_to_role": 0.85,
          "evidence": [
            {
              "source": "github_commit",
              "id": "commit-3",
              "snippet": "Migrate feature pipeline to new deployment infra",
              "timestamp": "2024-08-20"
            }
          ]
        }
      ],
      "growth_plan": [
        "Within 30 days, implement a PyTorch-based ranking model on AWS SageMaker using a public Learning-to-Rank dataset (e.g., MSLR-WEB30k), add pairwise hinge loss, and tune it until you achieve published SOTA NDCG@10; document the infra cost and share the repo for code-review feedback to demonstrate deep-learning-for-ranking proficiency.",
        "Over the next 6 weeks, build an end-to-end near-real-time feature pipeline with Flink on Kubernetes that ingests clickstream data, computes 50M event-level features daily, backfills with point-in-time correctness, and surfaces feature importance via SHAP; integrate it into your company’s search stack and run an A/B test that lifts CTR by at least 2% to showcase large-scale feature-engineering capability.",
        "In 60 days, design a query-embedding model using sentence-transformers fine-tuned on domain-specific query-doc pairs, evaluate it offline on MAP, nDCG@10, and query-cluster purity, then launch an online experiment measuring search-relevance metrics (precision@1, nDCG, abandonment rate) with 95% power; present the semantic-understanding insights and experiment results to the Search Science leadership team to prove readiness for Senior Applied Scientist scope."
      ]
    },
    {
      "name": "Charlie",
      "skills": [
        {
          "skill": "search relevance metrics & experimentation",
          "confidence": 0.8,
          "relevance_to_role": 0.9,
          "evidence": [
            {
              "source": "rfc",
              "id": "rfc-2",
              "snippet": "Defines metrics and pipelines for data processing",
              "timestamp": "2024-08-05"
            }
          ]
        },
        {
          "skill": "large-scale feature engineering",
          "confidence": 0.7,
          "relevance_to_role": 0.8,
          "evidence": [
            {
              "source": "github_commit",
              "id": "commit-5",
              "snippet": "feature-pipeline Refactor pipelines",
              "timestamp": "2024-07-30"
            }
          ]
        },
        {
          "skill": "search relevance metrics & experimentation",
          "confidence": 0.6,
          "relevance_to_role": 0.7,
          "evidence": [
            {
              "source": "jira_ticket",
              "id": "ticket-2",
              "snippet": "Resolve minor deployment issues affecting evaluation metrics",
              "timestamp": "2024-08-12"
            }
          ]
        }
      ],
      "growth_plan": [
        "Implement a transformer-based cross-encoder on a 1B+ query-doc pair dataset to boost NDCG@10 by 3% and write a tech report on hyper-parameter tuning for ranking",
        "Design a real-time feature pipeline that generates 500+ semantic & behavioral signals at 50k QPS using Flink and a vocabulary-pruned WordPiece tokenizer; benchmark latency vs. offline AUC",
        "Own an A/B test roadmap for search relevance that ties click-skip entropy, perceived-relevance human ratings and long-session revenue to ship a 0.5% CTR gain and present learnings to the Science & Product boards"
      ]
    }
  ],
  "domain_ownership": {
    "search relevance metrics & experimentation": [
      {
        "owner": "Alice",
        "confidence": 0.95,
        "relevance": 1.0
      },
      {
        "owner": "Alice",
        "confidence": 0.9,
        "relevance": 1.0
      },
      {
        "owner": "Bob",
        "confidence": 0.9,
        "relevance": 0.95
      },
      {
        "owner": "Charlie",
        "confidence": 0.8,
        "relevance": 0.9
      },
      {
        "owner": "Charlie",
        "confidence": 0.6,
        "relevance": 0.7
      }
    ],
    "large-scale feature engineering": [
      {
        "owner": "Alice",
        "confidence": 0.7,
        "relevance": 0.8
      },
      {
        "owner": "Bob",
        "confidence": 0.8,
        "relevance": 0.85
      },
      {
        "owner": "Charlie",
        "confidence": 0.7,
        "relevance": 0.8
      }
    ]
  },
  "collaboration_graph": {
    "Alice": {
      "Bob": 2,
      "Charlie": 2
    },
    "Bob": {
      "Alice": 2,
      "Charlie": 2
    },
    "Charlie": {
      "Alice": 2,
      "Bob": 2
    }
  },
  "risk_areas": [
    {
      "skill": "large-scale feature engineering",
      "severity": "LOW",
      "risk_score": 0.109,
      "owners": [
        "Alice",
        "Bob",
        "Charlie"
      ]
    },
    {
      "skill": "search relevance metrics & experimentation",
      "severity": "LOW",
      "risk_score": 0.046,
      "owners": [
        "Alice",
        "Alice",
        "Bob",
        "Charlie",
        "Charlie"
      ]
    }
  ],
  "critical_coverage": {
    "Alice": 2.41,
    "Bob": 1.535,
    "Charlie": 1.7
  },
  "generated_at": "2026-02-11 10:04:24.329917"
}